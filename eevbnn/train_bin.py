from .utils import (ensure_dir, get_nr_correct, default_dataset_root,
                    sqr_hinge_loss, torch_as_npy, ensure_training_state)
from .attack_impl import pgd_batch, loss_fn_map as pgd_loss_fn_map
from .mp_verifier_attack import VerifierAdvBatchFinder
from .net_tools import callib_bn
from . import net_bin

import torch
import torch.utils
import torch.nn as nn
import torch.optim as optim
import torchvision.utils as vutils
import numpy as np

import argparse
import time
import sys
import gc
import threading
from pathlib import Path
from tqdm import tqdm

train_loss_fn_map = {
    'xent': nn.CrossEntropyLoss,
    'sqr-hinge': lambda: sqr_hinge_loss,
}

logmsg = print

class WeightClip:
    _weights = None
    def __init__(self, model):
        weights = []
        for i in model.features:
            if isinstance(i, (net_bin.BinConv2d, net_bin.BinLinear)):
                weights.append(i.weight)
                if (mask := i._parameters.get('weight_mask')) is not None:
                    weights.append(mask)
        logmsg(f'WeightClip: {len(weights)} weight params')
        self._weights = weights

    def __call__(self):
        with torch.no_grad():
            for i in self._weights:
                i.clamp_(-1, 1)

class EpochHyperParam:
    __slots__ = [
        # epoch number
        'epoch',
        # learning rate
        'lr',
        # adversarial training perturbation bound / iteration steps
        'adv_epsilon', 'adv_steps',
        # activation stabilization coefficient (at most one enabled)
        'act_stabilize_abs', 'act_stabilize_smp',
        # ratio of adv images that should be generated by a verifier
        'verifier_mix',
        # whether BN should be freezed (i.e. set network in eval mode)
        'freeze_bn',
    ]

    @classmethod
    def from_epoch(cls, args, epoch):
        s = cls()
        s.epoch = epoch
        s.lr = args.lr * (0.5 ** (epoch // 150))
        p = {'epoch': epoch}
        s.adv_epsilon = float(eval(args.adv_epsilon, p))
        s.adv_steps = int(eval(args.adv_steps, p))
        s.act_stabilize_abs = float(eval(args.act_abs_stabilize, p))
        s.act_stabilize_smp = float(eval(args.act_smp_stabilize, p))
        s.verifier_mix = float(eval(args.verifier_mix, p))
        s.freeze_bn = eval(args.freeze_bn, p)
        assert type(s.freeze_bn) is bool
        assert (s.adv_epsilon >= 0 and s.adv_steps >= 0 and
                s.act_stabilize_abs >= 0 and s.act_stabilize_smp >= 0 and
                0 <= s.verifier_mix <= 1)
        assert not s.act_stabilize_abs or not s.act_stabilize_smp
        return s


class MinibatchState:
    __slots__ = [
        # total loss
        'loss',
        # classification loss
        'clsfy_loss',
        # string describing adversarial training
        'adv_str',
        # a function to get activation stabilization loss description
        'asloss_str_fn',
        # a function to get bias regularizer loss (a.k.a. CBD loss)
        'bloss_str_fn',
    ]

    def __init__(self):
        self.loss = 0
        self.adv_str = ''
        self.asloss_str_fn = lambda: ''
        self.bloss_str_fn = lambda: ''

    def setup_asloss(self, asloss: torch.Tensor, scale: torch.Tensor):
        asloss_s = scale * asloss
        self.loss = self.loss + asloss_s
        self.asloss_str_fn = lambda: (
            f'asloss={asloss_s.item():.2f}/{asloss.item():.2f} ')


class ModelChooser:
    _args = None
    _device = None
    _model = None
    _out_root = None

    _data = None
    _best_acc = 0

    def __init__(self, args, device, model, trainloader):
        self._args = args
        self._device = device
        self._model = model
        self._out_root = Path(args.output)
        self._data = [i for _, i in zip(range(args.choose_nr_minibatch),
                                        trainloader)]

    def enabled(self, epoch):
        return epoch >= self._args.epoch - self._args.choose_epoch

    @property
    def data(self):
        return self._data

    def __call__(self, ep: EpochHyperParam):
        epoch = ep.epoch
        if self._args.checkpoint and epoch % self._args.checkpoint == 0:
            self._model.save_to_file(self._out_root / f'chkpoint-{epoch}.pth')

        if not self.enabled(epoch):
            return

        self._model.save_to_file(self._out_root / f'epoch-{epoch}.pth')
        acc = self._eval_acc(ep)
        logmsg(f'model choosing acc @{epoch}: {acc*100:.2f}%')
        if acc > self._best_acc:
            logmsg(f'best model update at @{epoch}')
            self._best_acc = acc
            self._model.save_to_file(self._out_root / f'last.pth')

    def _eval_acc(self, ep):
        model = self._model
        model.eval()
        nr_correct = 0
        tot = 0
        for inputs, labels in self._data:
            inputs = inputs.to(self._device)
            labels = labels.to(self._device)

            if ep.adv_epsilon and ep.adv_steps:
                _, cnt = pgd_batch(
                    model, inputs, pgd_loss_fn_map[self._args.adv_loss](labels),
                    ep.adv_epsilon, ep.adv_steps, restore=True
                )
                nr_correct += cnt
            else:
                outputs = model(inputs)
                nr_correct += get_nr_correct(outputs, labels)

            tot += inputs.size(0)
        return nr_correct / tot


class GradScaleChooser:
    """set :attr:`~.Binarize01Act.grad_scale` values based to minimize adv
    acc; this should be called before epoch training starts"""
    _args = None
    _device = None
    _model = None

    _scale_params = None

    _data = None
    _adv_init_eps = None
    _adv_init = None

    _scale_cands = np.arange(0.6, 3.01, 0.4)
    """candidate scales"""

    def __init__(self, args, device, model, trainloader):
        self._args = args
        self._device = device
        self._model = model
        self._data = [i for _, i in zip(range(args.choose_nr_minibatch),
                                        trainloader)]

        self._scale_params = [i.grad_scale for i in model.features
                              if isinstance(i, net_bin.Binarize01Act)]

    def __call__(self, ep: EpochHyperParam):
        if not ep.adv_epsilon or not ep.adv_steps:
            return

        if ep.epoch < self._args.grad_scale_start_epoch:
            return

        if net_bin.g_bingrad_soft_tanh_scale is None:
            return

        if ep.adv_epsilon != self._adv_init_eps:
            self._adv_init = [
                torch.empty_like(i).
                uniform_(-ep.adv_epsilon, ep.adv_epsilon).
                add_(i)
                for i, _ in self._data
            ]
            self._adv_init_eps = ep.adv_epsilon

        scale_list = []  # (key, acc, scale)
        all_acc = []
        for scale in self._scale_cands:
            acc = self._eval_acc(scale, ep)
            all_acc.append(acc)
            scale_list.append([None, acc, scale])

        # merge close acc values
        all_acc.sort()
        acc2quant = {}
        prev = 0
        thresh = self._args.choose_nr_minibatch * self._args.batchsize // 1280
        for i in all_acc:
            if i - prev >= thresh:
                prev = i
            acc2quant[i] = prev - all_acc[0]
        for i in scale_list:
            i[0] = acc2quant[i[1]]

        # sort and select best
        scale_list.sort(key=lambda x: (x[0], abs(x[2] - 1)))
        scale = scale_list[0][2]
        self._set_scale(scale)
        scale_list_desc = ' '.join(
            f'{k:.1f}:({i},{j})' for i,j,k in scale_list
        )
        logmsg(
            f'set grad scale @{ep.epoch}: {scale=:.1f} in [{scale_list_desc}]'
        )

    def _set_scale(self, v: float):
        v = torch.tensor(float(v), device=self._device)
        for i in self._scale_params:
            i.copy_(v)

    def _eval_acc(self, scale, ep):
        self._set_scale(scale)
        ret = 0
        for (inputs, labels), adv_init in zip(self._data, self._adv_init):
            inputs = inputs.to(self._device)
            labels = labels.to(self._device)
            adv_init = adv_init.to(self._device)
            _, nr = call_pgd_batch(self._args, self._model, ep,
                                   inputs, labels, adv_init)
            ret += nr
        return ret


def call_pgd_batch(
        args, net, ep: EpochHyperParam,
        inputs: torch.Tensor, labels: torch.Tensor, adv_init):
    """call :func:`pgd_batch` in a safe environment"""
    if not ep.freeze_bn:
        net.eval_with_bn_(True)
    try:
        return pgd_batch(
            net, inputs, pgd_loss_fn_map[args.adv_loss](labels),
            ep.adv_epsilon, ep.adv_steps, restore=True,
            adv_init=adv_init,
        )
    finally:
        if not ep.freeze_bn:
            net.eval_with_bn_(False)

class TrainingLoop:
    _args = None
    _device = None
    _net = None

    _verifier_adv_finder = None

    _out_root = None

    _trainloader = None
    _testloader = None

    _model_chooser = None
    _grad_scale_chooser = None

    _criterion = None
    _optimizer = None
    _weight_clip = None
    _bias_regularizer = None

    _avg_test_acc = None

    _prev_end_time = None

    def __init__(self, args, device, net,
                 verifier_adv_finder: VerifierAdvBatchFinder):
        self._args = args
        self._device = device
        self._net = net
        self._verifier_adv_finder = verifier_adv_finder

        self._out_root = Path(args.output)

        self._trainloader = net.make_dataset_loader(args, True)
        self._testloader = net.make_dataset_loader(args, False)

        self._model_chooser = ModelChooser(
            args, device, net, self._trainloader)
        self._grad_scale_chooser = GradScaleChooser(
            args, device, net, self._trainloader)

        self._criterion = train_loss_fn_map[args.loss]()
        self._optimizer = optim.Adam(net.parameters(), lr=args.lr)
        self._weight_clip = WeightClip(net)
        self._bias_regularizer = net_bin.BiasRegularizer(
            args.bias_hinge_coeff, args.bias_hinge_thresh, net)

        self._avg_test_acc = []
        self._prev_end_time = time.time()

    def train_epoch(self, epoch):
        self._net.train()
        ep = EpochHyperParam.from_epoch(self._args, epoch)

        for param_group in self._optimizer.param_groups:
            param_group['lr'] = ep.lr

        msg = ''
        if ep.freeze_bn:
            msg += ' <freeze_bn>'
        if ep.adv_epsilon:
            msg += f' {ep.adv_epsilon=:.4f}'
        if ep.adv_steps:
            msg += f' {ep.adv_steps=}'

        logmsg(f'epoch: {epoch} {ep.lr=}{msg}')

        self._grad_scale_chooser(ep)

        for idx, (inputs, labels) in enumerate(self._trainloader):
            self._train_minibatch(
                idx, inputs.to(self._device), labels.to(self._device), ep)

        if (spar_stat := self._net.get_sparsity_stat()) is not None:
            spar_parts, spar_nz, spar_tot = spar_stat
            spar_parts = ', '.join('{:.2f}%'.format(i*100) for i in spar_parts)
            logmsg(f'sparsity@{epoch}: {spar_parts} ; '
                   f'{spar_nz}/{spar_tot}={spar_nz/spar_tot*100:.2f}%')

        if self._model_chooser.enabled(epoch):
            pre_bn_acc, _ = self._eval_model_on_test(record_avg=False)
            pre_bn_acc = f' pre_bn_acc={pre_bn_acc*100:.2f}%'
            callib_bn(self._args, self._net, self._device)
        else:
            pre_bn_acc = ''

        test_acc, test_loss = self._eval_model_on_test()
        logmsg(f'test@{epoch}: acc={test_acc*100:.2f}% '
               f'avg_acc={np.mean(self._avg_test_acc)*100:.2f}% '
               f'loss={test_loss:.2f}{pre_bn_acc}')

        self._model_chooser(ep)

    def _eval_model_on_test(self, record_avg=True):
        """:return: test accuracy, test loss"""
        nr_test_correct = 0
        nr_test_tot = 0
        test_loss_sum = 0
        self._net.eval()
        with torch.no_grad():
            for inputs, labels in self._testloader:
                inputs = inputs.to(self._device)
                labels = labels.to(self._device)
                outputs = self._net(inputs)
                loss = self._criterion(outputs, labels)
                size = int(inputs.size(0))
                test_loss_sum += float(loss.item()) * size
                nr_test_correct += get_nr_correct(outputs, labels)
                nr_test_tot += size

        test_acc = nr_test_correct / nr_test_tot
        if record_avg:
            avg = self._avg_test_acc
            if len(avg) == 10:
                del avg[0]
            avg.append(test_acc)

        return test_acc, test_loss_sum / nr_test_tot

    def _train_minibatch(self, idx, inputs, labels, ep: EpochHyperParam):
        ms = MinibatchState()

        if ep.freeze_bn:
            self._net.train()
            with torch.no_grad():
                self._net(inputs) # update bn param
            self._net.eval()

        inputs_adv = self._get_training_adv(inputs, labels, ep, ms)

        if not idx:
            vutils.save_image(inputs,
                              str(self._out_root / 'training_sample.png'),
                              normalize=False)
            if inputs_adv is not inputs:
                vutils.save_image(
                    inputs_adv,
                    str(self._out_root / 'training_sample_adv.png'),
                    normalize=False)

        time_data = time.time() - self._prev_end_time

        self._optimizer.zero_grad()
        outputs = self._compute_losses(inputs, inputs_adv, labels, ep, ms)
        ms.loss.backward()
        self._optimizer.step()
        self._weight_clip()

        time_train = time.time() - self._prev_end_time

        nr_correct_cache = None
        def nr_correct():
            nonlocal nr_correct_cache
            if nr_correct_cache is None:
                nr_correct_cache = get_nr_correct(outputs, labels)
            return nr_correct_cache

        if idx % 20 == 0:
            acc = nr_correct() / inputs.size(0)
            if ms.adv_str:
                ms.adv_str += f':{inputs.size(0)-nr_correct()} '
            logmsg(
                f'[{ep.epoch}, {idx}] '
                f'loss={ms.clsfy_loss.item():.2f}/{ms.loss.item():.2f} '
                f'{ms.bloss_str_fn()}{ms.asloss_str_fn()}{ms.adv_str}'
                f'acc={acc*100:.2f}% '
                f'tdata/train=({time_data:.3f},{time_train:.3f})')

        self._prev_end_time = time.time()

    def _compute_losses(self, inputs, inputs_adv, labels, ep, ms):
        with self._bias_regularizer:
            if ep.act_stabilize_smp and ep.adv_epsilon:
                outputs, asloss = self._net.forward_with_multi_sample(
                    inputs, inputs_adv, ep.adv_epsilon)
                ms.setup_asloss(asloss, ep.act_stabilize_smp)
            else:
                outputs = self._net(inputs_adv)

        ms.clsfy_loss = self._criterion(outputs, labels)
        ms.loss += ms.clsfy_loss

        if self._bias_regularizer.coeff:
            ms.loss += self._bias_regularizer.loss
            ms.bloss_str_fn = lambda: (
                f'bloss={self._bias_regularizer.loss.item():.2f}/'
                f'{self._bias_regularizer.loss_max.item():.2f} ')

        if ep.act_stabilize_abs and ep.adv_epsilon:
            ms.setup_asloss(
                self._net.compute_act_stabilizing_loss_abstract(
                    inputs, ep.adv_epsilon),
                ep.act_stabilize_abs
            )

        return outputs

    def _get_training_adv(self, inputs, labels,
                          ep: EpochHyperParam, ms: MinibatchState):
        if ep.adv_epsilon <= 1e-9:
            return inputs

        net = self._net

        def run_pgd(x0, adv_init):
            if not ep.adv_steps:
                if adv_init is not None:
                    return adv_init
                return x0
            ret, nr_correct = call_pgd_batch(self._args, net, ep,
                                             x0, labels, adv_init)
            if not ms.adv_str:
                ms.adv_str = f'pgd={x0.size(0)-nr_correct}'
            return ret

        def run_verifier():
            nonlocal inputs_adv
            nr_adv_goal = max(
                int(round(ep.verifier_mix * inputs.shape[0])), 1)
            inputs_adv, nr_adv_verifier, nr_adv_ref = (
                self._verifier_adv_finder.find_adv_batch(
                    net, inputs, inputs_adv, labels, ep.adv_epsilon,
                    nr_adv_goal))
            ms.adv_str = (
                f'vrf=({nr_adv_verifier}/{nr_adv_goal}+{nr_adv_ref})')

        inputs_adv = run_pgd(inputs, None)

        if ep.verifier_mix > 0:
            bn_state = net.get_bn_state()
            try:
                for i in net.features:
                    if isinstance(i, net_bin.BatchNormStatsCallbak):
                        i.momentum = 0
                with ensure_training_state(net, True):
                    # set the statistics to current batch so verifier works
                    # better
                    net(inputs_adv)

                run_verifier()
            finally:
                net.restore_bn_state(bn_state)

            # optimize the adv starting from verifier results
            inputs_adv = run_pgd(inputs, inputs_adv)


        return inputs_adv


def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('net', help='name of network')
    parser.add_argument('output')
    parser.add_argument('--load', help='load saved network')
    parser.add_argument('--wait-load', action='store_true',
                        help='wait for the file specified in --load to become '
                        'available')
    parser.add_argument('--start-epoch', type=int, default=0,
                        help='starting epoch number, for correctly '
                        'continuing training')
    parser.add_argument('--epoch', type=int, default=200)
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='learning rate')
    parser.add_argument('--batchsize', type=int, default=128)
    parser.add_argument('--workers', type=int, default=2,
                        help='number of CPU workers for data loading')
    parser.add_argument(
        '--data', default=default_dataset_root(),
        help='dir for training data')
    parser.add_argument(
        '--bias-hinge-thresh', type=float, default=0,
        help='threshold for hinge loss of bias'
    )
    parser.add_argument(
        '--bias-hinge-coeff', type=float, default=0,
        help='coefficient for hinge loss of bias'
    )
    parser.add_argument('--input-quantization', type=float, default=1/255,
                        help='input quantization step on 0-1 scale')
    parser.add_argument('--adv-steps', default='0',
                        help='a python expression for PGD steps for '
                        'adversarial training')
    parser.add_argument('--adv-epsilon', default='0',
                        help='a python expression to set adv epsilon for each '
                        'epoch')
    parser.add_argument('--adv-loss', default='xent',
                        choices=pgd_loss_fn_map.keys(),
                        help='loss function for adversarial example mining')
    parser.add_argument('--act-abs-stabilize', default='0',
                        help='a python expression for action stablization '
                        'coeff by abstract interpretation')
    parser.add_argument('--act-smp-stabilize', default='0',
                        help='a python expression for action stablization '
                        'coeff by multiple sampling')
    parser.add_argument('--loss', choices=train_loss_fn_map.keys(),
                        default='xent',
                        help='classification loss function for training')
    parser.add_argument('--set-global-param', default=[], action='append',
                        help='set global parameter in the net_bin module')
    parser.add_argument('--choose-nr-minibatch', type=int, default=40,
                        help='number of minibatches from training data to '
                        'choose best model and grad scale factor')
    parser.add_argument('--choose-epoch', type=int, default=3,
                        help='last epochs to enter best model choice')
    parser.add_argument('--verifier-nproc', type=int, default=4,
                        help='number of processes to run the verifier')
    parser.add_argument('--grad-scale-start-epoch', type=int, default=15,
                        help='epoch starting at which gradient scale should be '
                        'adjusted')
    parser.add_argument(
        '--verifier-mix', default='0',
        help='a python expression for max number of adversarial examples from '
        'verifier; use 0 to disable verifier. It is a value between [0, 1] '
        'relative to the batch size')
    parser.add_argument(
        '--freeze-bn', default='False',
        help='a python expression to set whether BN should be freezed')
    parser.add_argument('--checkpoint', type=int, default=0,
                        help='epochs to save checkpoint')
    args = parser.parse_args()

    for i in args.set_global_param:
        k, v = i.split('=')
        assert k.startswith('g_') and hasattr(net_bin, k), (
            f'no such global param {k}')
        setattr(net_bin, k, eval(v, {'net_bin': net_bin}))

    ensure_dir(args.output)

    net_class = getattr(net_bin, args.net)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    while args.wait_load and not Path(args.load).exists():
        print(f'{time.time():.3f}: file '
              f'{args.load} does not exist; waiting ...', flush=True)
        time.sleep(2)
    net = net_class.create_with_load(
        fpath=args.load,
        kwargs={'quant_step': args.input_quantization},
        enforce_state_load=True).to(device)

    out_root = Path(args.output)
    with (out_root / 'log.txt').open('w') as fout:
        pass
    def logmsg_(msg):
        print(msg)
        with (out_root / 'log.txt').open('a') as fout:
            print(msg, file=fout)
    global logmsg
    logmsg = logmsg_

    logmsg(f'argv: {sys.argv}')
    logmsg(f'parsed args: {args}')
    logmsg('global params: {}'.format(
        [(k, v) for k, v in net_bin.__dict__.items()
         if k.startswith('g_')]))
    logmsg(f'network: {net}')

    verifier_adv_finder = VerifierAdvBatchFinder(args.verifier_nproc)
    trainer = TrainingLoop(args, device, net, verifier_adv_finder)
    try:
        for i in range(args.start_epoch, args.epoch):
            trainer.train_epoch(i)
    finally:
        verifier_adv_finder.close()

    with (out_root / 'finish_mark').open('w') as fout:
        pass

if __name__ == '__main__':
    main()
